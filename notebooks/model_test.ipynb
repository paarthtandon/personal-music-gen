{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\Paarth Tandon\\\\Desktop\\\\repos\\\\personal-music-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "c:\\Users\\Paarth Tandon\\Desktop\\repos\\personal-music-gen\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from personal_musicgen.data.datasets import AudioDataset\n",
    "\n",
    "data = AudioDataset('../data/chunks_original/', no_label=False)\n",
    "model = MusicGen.get_pretrained('small')\n",
    "model.lm = model.lm.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/chunks_original/animalsasleadersonimpulse_chunk_10_original.wav',\n",
       " '../data/chunks_original/animalsasleadersonimpulse_chunk_10_original.txt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def preprocess_audio(audio_path, model: MusicGen, duration: int = 30):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = torchaudio.functional.resample(wav, sr, model.sample_rate)\n",
    "    wav = wav.mean(dim=0, keepdim=True)\n",
    "    if wav.shape[1] < model.sample_rate * duration:\n",
    "        return None\n",
    "    end_sample = int(model.sample_rate * duration)\n",
    "    start_sample = random.randrange(0, max(wav.shape[1] - end_sample, 1))\n",
    "    wav = wav[:, start_sample : start_sample + end_sample]\n",
    "\n",
    "    assert wav.shape[0] == 1\n",
    "\n",
    "    wav = wav.cuda()\n",
    "    wav = wav.unsqueeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "\n",
    "    codes, scale = gen_audio\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, _ = torchaudio.load(data[100][0])\n",
    "orig = orig.mean(dim=0, keepdim=True)\n",
    "orig = orig.unsqueeze(1).cuda()\n",
    "orig, _ = model.compression_model.encode(orig)\n",
    "\n",
    "prep = preprocess_audio(data[100][0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 1500]), torch.Size([1, 4, 1500]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig.shape, prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [ConditioningAttributes(text={'description': 'prompt'}, wav={'self_wav': WavCondition(wav=tensor([[[0.]]], device='cuda:0'), length=tensor([0], device='cuda:0'), sample_rate=[32000], path=[None], seek_time=[])}, joint_embed={})])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes, _ = model._prepare_tokens_and_attributes(['prompt'], None)\n",
    "len(attributes), attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': (tensor([[[ 0.2329,  0.5538,  0.3575,  ..., -0.1976, -0.2641, -0.5404],\n",
       "           [-0.0173, -0.0040,  0.0010,  ...,  0.0045,  0.0052,  0.0121]],\n",
       "  \n",
       "          [[-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n",
       "         device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[1, 1],\n",
       "          [0, 0]], device='cuda:0'))}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "\n",
    "def get_condition_tensor(model, attributes) -> torch.Tensor:\n",
    "    null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(attributes)\n",
    "    conditions = attributes + null_conditions\n",
    "    tokenized = model.lm.condition_provider.tokenize(conditions)\n",
    "    cfg_conditions = model.lm.condition_provider(tokenized)\n",
    "    return cfg_conditions\n",
    "\n",
    "condition_tensors = get_condition_tensor(model, attributes)\n",
    "condition_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(condition_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([4, 1500, 2048]) torch.Size([4, 1500, 2048]) torch.Size([4, 1500])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    lm_output = model.lm.compute_predictions(\n",
    "        codes=orig, conditions=[], condition_tensors=condition_tensors\n",
    "    )\n",
    "    print(lm_output.logits.device)\n",
    "\n",
    "    codes = orig[0]\n",
    "    logits = lm_output.logits[0]\n",
    "    mask = lm_output.mask[0]\n",
    "\n",
    "    codes = torch.nn.functional.one_hot(codes, 2048).type(logits.dtype)\n",
    "    print(codes.shape, logits.shape, mask.shape)\n",
    "\n",
    "    codes = codes.cuda()\n",
    "    logits = logits.cuda()\n",
    "    mask = mask.cuda()\n",
    "    mask = mask.view(-1)\n",
    "    masked_logits = logits.view(-1, 2048)[mask]\n",
    "    masked_codes = codes.view(-1, 2048)[mask]\n",
    "    loss = criterion(masked_logits, masked_codes)\n",
    "    print(type(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
